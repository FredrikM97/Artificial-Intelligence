title: Algorithms

# Chapter 3 (Algorithms)

Search algorithms treat states and actions asatomic:  they do not consider any internalstructure they might possess.

A general TREE-SEARCH algorithm considers all possible paths to find  a solution, whereas a GRAPH-SEARCH algorithm avoids consideration of redundant paths.

Search algorithms are judged on the basis ofcompleteness,optimality,time complex-ity,andspace complexity. Complexity depends on $b$, the branching factor in the statespace, andd, the depth of the shallowest solution

## Uniformed search (blind search)
BFS (Breadth-first search)
  
* Expands the shallowest nodes first;  it is complete,  optimal for unit step costs, but has exponential space complexity. FIFO queue, Not recommended for big scale with a lot of nodes
* Complete: Yes, Time: $O(b^d)$, Space: $O(b^d)$, Optimal: Yes

Uniform-cost search
  
* Expands the node with lowest path cost,g(n), and is optimal for general step cost
* Doesnt care about step cost but the total, can therefor be stuck in inf loop if inf number of nodes with 0 cost
* Complete: Yes, Time: $O(b^{1+[C^*/\epsilon]})$, Space: $O(b^{1+[C^*/\epsilon]})$, Optimal: Yes

DFS (Depth-first search)
  
* Expands the deepest unexpanded node first.  It is neither complete nor optimal, but has linear space complexity. Depth-limited searchadds adepth bound. 
* Alternative to Graph-search-implementation
* LIFO
* Common to implement with recursive function.
* Fails on tree and graph structure if inf non-goal path is encountered. Will search though all paths even if they arent the goal
* Better space complexity in tree search than BFS. Removing finished branches after check if that wasnt the goal
* Complete: No, Time: $O(b^m)$, Space: $O(bm)$, Optimal: No
* 
IDS (terative deepening search)
  
*  Calls depth-first search with increasing  depth limits until a goal is found. It is complete, optimal for unit step costs, has time complexity comparable to breadth-first search, and has linear space complexity.
* Complete: Yes, Time: $O(b^d)$, Space: $O(bd)$, Optimal: Yes  

## Informed Search
GBFS (Greedy best-first search)
* Expand node thath is closest to the goal
* Expands nodes with minimal $h(n)$. It is not optimal but is often efficient.
* Worst case $O(b^m)$ time/space complexy. $m: max depth for search space$

$A^*$ ($A^*$ search)
* Evaluates nodes by combining $g(n)$, the cost to reach the node, and $h(n)$, the cost to get from the node to the goal:$f(n)=g(n)+h(n)$  
* $A^*$ is complete and optimal, provided that $h(n)$ is admissible (for TREE-SEARCH) or consistent (for GRAPH-SEARCH). The space complexity of A∗is still prohibitive.
* $A^*$ optimal if $h(n)$ is admissible, graph.search if $h(n)$ is conisistent


# Chapter 4 ()

Local search methods such as hill climbing operate  on complete-state  formulations,keeping only a small number of nodes in memory.  Several stochastic algorithms havebeen developed, includingsimulated annealing, which returns optimal solutions whengiven an appropriate cooling schedule.

## Hill-climbing:
* Simply a loop that continually moves in the direction of increasing value—that is, uphill.  Itterminates when it reaches a “peak” where no neighbor has a higher value.
* No record of search tree, only state and value for objective function
* This resembles trying to find the top of MountEverest in a thick fog while suffering from amnesia.
* Sometimes called Gready local search: Grabs a good neighbor state without thinking ahead

Stochastic hill climbing
* Chooses at STOCHASTIC HILL CLIMBING random from among the uphill moves; the probability of selection can vary with the steepnessof the uphill move.  

First-choice hill climbing
* Implements stochastic FIRST-CHOICE HILL CLIMBING hill climbing by generating successors randomly until one is generated that is better than thecurrent state.

Random-restart hillclimbing
* Adopts the well-known adage, “If at first you don’t succeed, try, try again.”

Simulated annealing
* Combine hill climbing + random

Beam search
* Remember k states instead of just one

## Summery
Many local search methods apply also to problems in continuous spaces.Linear pro-grammingandconvex optimizationproblems obey certain restrictions on the shapeof the state space and the nature of the objective function, and admit polynomial-timealgorithms that are often extremely efficient in practice.

Agenetic algorithmis a stochastic hill-climbing search in which a large population of states is maintained.  New states are generated by mutation and bycrossover,which combines pairs of states from the population

In nondeterministic environments, agents can apply AND–OR search to generate con-tingent plans that reach the goal regardless of which outcomes occur during execution.

When the environment is partially observable, the belief state represents the set of possible states that the agent might be in.

Standard search algorithms can be applied directly to belief-state space to solvesensor-less problems, and belief-stateAND–ORsearch can solve general partially observableproblems. Incremental algorithms that construct solutions state-by-state within a beliefstate are often more efficient.

Exploration problemsarise when the agent has no idea about the states and actions ofits environment.  For safely explorable environments,online searchagents can build amap and find a goal if one exists. Updating heuristic estimates from experience providesan effective method to escape from local minima

## Generic algorithms
* Variant of stochastic beam search in which successor statesGENETICALGORITHMare generated by combiningtwoparent states rather than by modifying a single state



# Chapter 5

## Minimax
Computes minimax decision from the current state.
Uses simple recursive compitation of the minimax values of each successor state.
Recursion proceeds down to the elaves and then minimax values backed up though the tree

Depth-first exploration of game tree. If max deph is m and b legal moves at each point: TIme complexity. $O(b^m)$ and space complexity $O(bm)$ for an algorithm that generates all action at once. Or $O(m)$ for actions one at a time.

In two-player zero-sum games withperfect information,theminimaxalgorithm canselect optimal moves by a depth-first enumeration of the game tree

## Alpha beta pruning
The alpha–beta search algorithm computes the same optimal move as minimax, but achieves much greater efficiency by eliminating subtrees that are provably irrelevant (cant influence the final decision).


Usually, it is not feasible to consider the whole game tree (even with alpha–beta), so we need to cut the search off at some point and apply a heuristic evaluation function that estimates the utility of a state.

$\alpha$: The highest value found at any point along the max 
$\beta$: The lowest value found at any point along the min

Best case $O(b^{m/2})$
## Sum
* Games of chance can be handled by an extension to the minimax algorithm that eval-uates achance nodeby taking the average utility of all its children,  weighted by theprobability of each child.

* Optimal play in games of imperfect information, such as Kriegspiel and bridge, re-quires reasoning  about  the current  and future belief statesof each player.   A simple approximation can be obtained by averaging the value of an action over each possible configuration of missing information.
* Programs have bested even champion human players at games such as chess, checkers,and Othello.  Humans retain the edge in several games of imperfect information, such as poker,  bridge,  and Kriegspiel,  and in games with very large branching factors and little good heuristic knowledge, such as Go.

## Evaluationfunction
EVA L(s)=w1f1(s)+w2f2(s) -- sum[w_if_i(s)] were w2 could be the worth of a chess and f how many there is
## Reinforcement learning

## Support vector machine

#

